---
---

@string{aps = {American Physical Society,}}

@inproceedings{li-etal-2019-word-segmentation,
title = "Is Word Segmentation Necessary for Deep Learning of {C}hinese Representations?",
author = "Li, Xiaoya  and
      Meng, Yuxian  and
      Sun, Xiaofei  and
      Han, Qinghong  and
      Yuan, Arianna  and
      Li, Jiwei",
booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
month = jul,
year = "2019",
address = "Florence, Italy",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/P19-1314",
doi = "10.18653/v1/P19-1314",
pages = "3242--3252",
abstract = "Segmenting a chunk of text into words is usually the first step of processing Chinese text, but its necessity has rarely been explored. In this paper, we ask the fundamental question of whether Chinese word segmentation (CWS) is necessary for deep learning-based Chinese Natural Language Processing. We benchmark neural word-based models which rely on word segmentation against neural char-based models which do not involve word segmentation in four end-to-end NLP benchmark tasks: language modeling, machine translation, sentence matching/paraphrase and text classification. Through direct comparisons between these two types of models, we find that char-based models consistently outperform word-based models. Based on these observations, we conduct comprehensive experiments to study why word-based models underperform char-based models in these deep learning-based NLP tasks. We show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting. We hope this paper could encourage researchers in the community to rethink the necessity of word segmentation in deep learning-based Chinese Natural Language Processing.",
}

@article{meng2019glyce,
title={Glyce: Glyph-vectors for chinese character representations},
author={Meng, Yuxian and Wu, Wei and Wang, Fei and Li, Xiaoya and Nie, Ping and Yin, Fan and Li, Muyu and Han, Qinghong and Sun, Xiaofei and Li, Jiwei},
journal={Advances in Neural Information Processing Systems},
volume={32},
year={2019}
}

@article{li2020sac,
title={Sac: Accelerating and structuring self-attention via sparse adaptive connection},
author={Li, Xiaoya and Meng, Yuxian and Zhou, Mingxin and Han, Qinghong and Wu, Fei and Li, Jiwei},
journal={Advances in Neural Information Processing Systems},
volume={33},
pages={16997--17008},
year={2020}
}

@inproceedings{li-etal-2020-unified,
title = "A Unified {MRC} Framework for Named Entity Recognition",
author = "Li, Xiaoya  and
      Feng, Jingrong  and
      Meng, Yuxian  and
      Han, Qinghong  and
      Wu, Fei  and
      Li, Jiwei",
booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
month = jul,
year = "2020",
address = "Online",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2020.acl-main.519",
doi = "10.18653/v1/2020.acl-main.519",
pages = "5849--5859",
abstract = "The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not.Models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat NER, are only able to assign a single label to a particular token, which is unsuitable for nested NER where a token may be assigned several labels. In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks. Instead of treating the task of NER as a sequence labeling problem, we propose to formulate it as a machine reading comprehension (MRC) task. For example, extracting entities with the per label is formalized as extracting answer spans to the question {``}\textit{which person is mentioned in the text}''.This formulation naturally tackles the entity overlapping issue in nested NER: the extraction of two overlapping entities with different categories requires answering two independent questions. Additionally, since the query encodes informative prior knowledge, this strategy facilitates the process of entity extraction, leading to better performances for not only nested NER, but flat NER. We conduct experiments on both nested and flat NER datasets.Experiment results demonstrate the effectiveness of the proposed formulation. We are able to achieve a vast amount of performance boost over current SOTA models on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37,respectively on ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets, i.e., +0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA and Chinese OntoNotes 4.0.",
}

@inproceedings{li-etal-2020-dice,
title = "Dice Loss for Data-imbalanced {NLP} Tasks",
author = "Li, Xiaoya  and
      Sun, Xiaofei  and
      Meng, Yuxian  and
      Liang, Junjun  and
      Wu, Fei  and
      Li, Jiwei",
booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
month = jul,
year = "2020",
address = "Online",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2020.acl-main.45",
doi = "10.18653/v1/2020.acl-main.45",
pages = "465--476",
abstract = "Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the S{\o}rensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.",
}


@inproceedings{meng-etal-2022-fast,
title = "Fast Nearest Neighbor Machine Translation",
author = "Meng, Yuxian  and
      Li, Xiaoya  and
      Zheng, Xiayu  and
      Wu, Fei  and
      Sun, Xiaofei  and
      Zhang, Tianwei  and
      Li, Jiwei",
booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
month = may,
year = "2022",
address = "Dublin, Ireland",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2022.findings-acl.47",
doi = "10.18653/v1/2022.findings-acl.47",
pages = "555--565",
abstract = "Though nearest neighbor Machine Translation ($k$NN-MT) (CITATION) has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search. This means each step for each beam in the beam search has to search over the entire reference corpus. $k$NN-MT is thus two-orders slower than vanilla MT models, making it hard to be applied to real-world applications, especially online services. In this work, we propose Fast $k$NN-MT to address this issue. Fast $k$NN-MT constructs a significantly smaller datastore for the nearest neighbor search: for each word in a source sentence, Fast $k$NN-MT first selects its nearest token-level neighbors, which is limited to tokens that are the same as the query token. Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens. This strategy avoids search through the whole datastore for nearest neighbors and drastically improves decoding efficiency. Without loss of performance, Fast $k$NN-MT is two-orders faster than $k$NN-MT, and is only two times slower than the standard NMT model. Fast $k$NN-MT enables the practical use of $k$NN-MT systems in real-world MT applications. The code is available at \url{https://github.com/ShannonAI/fast-knn-nmt}.",
}

@inproceedings{gan-etal-2022-dependency,
title = "Dependency Parsing as {MRC}-based Span-Span Prediction",
author = "Gan, Leilei  and
      Meng, Yuxian  and
      Kuang, Kun  and
      Sun, Xiaofei  and
      Fan, Chun  and
      Wu, Fei  and
      Li, Jiwei",
booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
month = may,
year = "2022",
address = "Dublin, Ireland",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2022.acl-long.173",
doi = "10.18653/v1/2022.acl-long.173",
pages = "2427--2437",
abstract = "Higher-order methods for dependency parsing can partially but not fully address the issue that edges in dependency trees should be constructed at the text span/subtree level rather than word level. In this paper, we propose a new method for dependency parsing to address this issue. The proposed method constructs dependency trees by directly modeling span-span (in other words, subtree-subtree) relations. It consists of two modules: the \textit{text span proposal module} which proposes candidate text spans, each of which represents a subtree in the dependency tree denoted by (root, start, end); and the \textit{span linking module}, which constructs links between proposed spans. We use the machine reading comprehension (MRC) framework as the backbone to formalize the span linking module, where one span is used as query to extract the text span/subtree it should be linked to. The proposed method has the following merits: (1) it addresses the fundamental problem that edges in a dependency tree should be constructed between subtrees; (2) the MRC framework allows the method to retrieve missing spans in the span proposal stage, which leads to higher recall for eligible spans. Extensive experiments on the PTB, CTB and Universal Dependencies (UD) benchmarks demonstrate the effectiveness of the proposed method. The code is available at \url{https://github.com/ShannonAI/mrc-for-dependency-parsing}",
}

@article{meng2022gnn,
title={Gnn-lm: Language modeling based on global contexts via gnn},
author={Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
journal={International Conference on Learning Representations},
year={2022}
}

@article{chen2022badpre,
title={Badpre: Task-agnostic backdoor attacks to pre-trained nlp foundation models},
author={Chen, Kangjie and Meng, Yuxian and Sun, Xiaofei and Guo, Shangwei and Zhang, Tianwei and Li, Jiwei and Fan, Chun},
journal={International Conference on Learning Representations},
year={2022}
}

@inproceedings{meng-etal-2021-conrpg,
title = "{C}on{RPG}: Paraphrase Generation using Contexts as Regularizer",
author = "Meng, Yuxian  and
      Ao, Xiang  and
      He, Qing  and
      Sun, Xiaofei  and
      Han, Qinghong  and
      Wu, Fei  and
      Fan, Chun  and
      Li, Jiwei",
booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
month = nov,
year = "2021",
address = "Online and Punta Cana, Dominican Republic",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2021.emnlp-main.199",
doi = "10.18653/v1/2021.emnlp-main.199",
pages = "2551--2562",
abstract = "A long-standing issue with paraphrase generation is the lack of reliable supervision signals. In this paper, we propose a new unsupervised paradigm for paraphrase generation based on the assumption that the probabilities of generating two sentences with the same meaning given the same context should be the same. Inspired by this fundamental idea, we propose a pipelined system which consists of paraphrase candidate generation based on contextual language models, candidate filtering using scoring functions, and paraphrase model training based on the selected candidates. The proposed paradigm offers merits over existing paraphrase generation methods: (1) using the context regularizer on meanings, the model is able to generate massive amounts of high-quality paraphrase pairs; (2) the combination of the huge amount of paraphrase candidates and further diversity-promoting filtering yields paraphrases with more lexical and syntactic diversity; and (3) using human-interpretable scoring functions to select paraphrase pairs from candidates, the proposed framework provides a channel for developers to intervene with the data generation process, leading to a more controllable model. Experimental results across different tasks and datasets demonstrate that the proposed paradigm significantly outperforms existing paraphrase approaches in both supervised and unsupervised setups.",
}
